id: flux2-klein-9b
name: "FLUX.2 Klein 9B"
type: diffusion_model
architecture: flux
author: unsloth / shimmyshimmer
license: flux-2-klein-non-commercial
homepage: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF
description: |
  FLUX.2 Klein 9B from Unsloth, available as GGUF quantizations.
  Successor to FLUX.1 with improved image quality and editing capabilities.
  Supports text-to-image and image editing workflows.
  Uses FLUX.2 VAE and Qwen 3 8B text encoder.
  Requires ComfyUI-GGUF custom node for GGUF variants.

variants:
  - id: bf16
    file: flux-2-klein-9b-BF16.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-BF16.gguf
    sha256: "VERIFY_flux2_klein_bf16"
    size: 18200000000
    format: gguf
    precision: bf16
    vram_required: 20480
    vram_recommended: 24576
    note: "Full precision bf16 GGUF. Best quality."

  - id: f16
    file: flux-2-klein-9b-F16.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-F16.gguf
    sha256: "VERIFY_flux2_klein_f16"
    size: 18200000000
    format: gguf
    precision: f16
    vram_required: 20480
    vram_recommended: 24576
    note: "Full precision fp16 GGUF."

  - id: gguf-q8-0
    file: flux-2-klein-9b-Q8_0.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q8_0.gguf
    sha256: "VERIFY_flux2_klein_q8_0"
    size: 9980000000
    format: gguf
    precision: q8_0
    vram_required: 12288
    vram_recommended: 16384
    note: "GGUF Q8_0 — highest quality quantization."

  - id: gguf-q6-k
    file: flux-2-klein-9b-Q6_K.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q6_K.gguf
    sha256: "VERIFY_flux2_klein_q6_k"
    size: 7870000000
    format: gguf
    precision: q6_k
    vram_required: 10240
    vram_recommended: 12288
    note: "GGUF Q6_K — very good quality/size balance."

  - id: gguf-q5-k-m
    file: flux-2-klein-9b-Q5_K_M.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q5_K_M.gguf
    sha256: "VERIFY_flux2_klein_q5_k_m"
    size: 7020000000
    format: gguf
    precision: q5_k_m
    vram_required: 8192
    vram_recommended: 12288
    note: "GGUF Q5_K_M — recommended for 12GB+ GPUs."

  - id: gguf-q5-k-s
    file: flux-2-klein-9b-Q5_K_S.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q5_K_S.gguf
    sha256: "VERIFY_flux2_klein_q5_k_s"
    size: 6940000000
    format: gguf
    precision: q5_k_s
    vram_required: 8192
    vram_recommended: 12288
    note: "GGUF Q5_K_S — smaller Q5 variant."

  - id: gguf-q4-k-m
    file: flux-2-klein-9b-Q4_K_M.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_K_M.gguf
    sha256: "VERIFY_flux2_klein_q4_k_m"
    size: 5910000000
    format: gguf
    precision: q4_k_m
    vram_required: 8192
    vram_recommended: 10240
    note: "GGUF Q4_K_M — good for 8GB GPUs."

  - id: gguf-q4-k-s
    file: flux-2-klein-9b-Q4_K_S.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_K_S.gguf
    sha256: "VERIFY_flux2_klein_q4_k_s"
    size: 5830000000
    format: gguf
    precision: q4_k_s
    vram_required: 8192
    vram_recommended: 10240
    note: "GGUF Q4_K_S — smaller Q4 variant."

  - id: gguf-q4-1
    file: flux-2-klein-9b-Q4_1.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_1.gguf
    sha256: "VERIFY_flux2_klein_q4_1"
    size: 6160000000
    format: gguf
    precision: q4_1
    vram_required: 8192
    vram_recommended: 10240
    note: "GGUF Q4_1."

  - id: gguf-q4-0
    file: flux-2-klein-9b-Q4_0.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q4_0.gguf
    sha256: "VERIFY_flux2_klein_q4_0"
    size: 5620000000
    format: gguf
    precision: q4_0
    vram_required: 8192
    vram_recommended: 10240
    note: "GGUF Q4_0."

  - id: gguf-q3-k-m
    file: flux-2-klein-9b-Q3_K_M.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q3_K_M.gguf
    sha256: "VERIFY_flux2_klein_q3_k_m"
    size: 4770000000
    format: gguf
    precision: q3_k_m
    vram_required: 6144
    vram_recommended: 8192
    note: "GGUF Q3_K_M — for 8GB GPUs. Some quality loss."

  - id: gguf-q3-k-s
    file: flux-2-klein-9b-Q3_K_S.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q3_K_S.gguf
    sha256: "VERIFY_flux2_klein_q3_k_s"
    size: 4690000000
    format: gguf
    precision: q3_k_s
    vram_required: 6144
    vram_recommended: 8192
    note: "GGUF Q3_K_S — smaller Q3 variant."

  - id: gguf-q2-k
    file: flux-2-klein-9b-Q2_K.gguf
    url: https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/flux-2-klein-9b-Q2_K.gguf
    sha256: "VERIFY_flux2_klein_q2_k"
    size: 3980000000
    format: gguf
    precision: q2_k
    vram_required: 6144
    vram_recommended: 8192
    note: "GGUF Q2_K — worst quality but smallest. Noticeable degradation."

requires:
  - id: flux2-vae
    type: vae
    reason: "FLUX.2 VAE for decoding latents to images"
  - id: flux2-qwen3-8b-text-encoder
    type: text_encoder
    reason: "Qwen 3 8B text encoder for prompt processing"

tags:
  - flux
  - text-to-image
  - image-editing
  - gguf
  - comfyui
  - unsloth

rating: 4.7
downloads: 0
added: "2025-07-01"
updated: "2025-07-01"

preview_images:
  - https://huggingface.co/unsloth/FLUX.2-klein-9B-GGUF/resolve/main/assets/flux2klein9b.png
